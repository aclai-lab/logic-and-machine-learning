{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"..\")\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "Random.seed!(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa6ac6",
   "metadata": {},
   "source": [
    "### Many-Expert Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24135d71",
   "metadata": {},
   "source": [
    "`ManyExpertDecisionTrees.jl` is still in development and has not been released\n",
    "yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb397db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ManyExpertDecisionTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b8092a",
   "metadata": {},
   "source": [
    "\"Many-Expert Decision Trees\" sounds like a very general name...\n",
    "\n",
    "Let's start with some motivation (and only one expert):\n",
    "- we want to move from \"hard\" to \"soft\" decisions\n",
    "- we want a better treatment of uncertainty\n",
    "\n",
    "How will we achieve that?\n",
    "- evaluating all the branches in our tree, and choosing the one(s) with higher\n",
    "values - i.e., we do not comply to one, strict, crisp decision at each step,\n",
    "but we take into consideration the contribution of each node\n",
    "- for each node, we won't have that a feature is \"true\" or \"false\"; rather, we\n",
    "will assign a value between 0 and 1, and combine these values using the t-norm\n",
    "- at the end, we do not constraint the model to always give a (single) class - \n",
    "it can also say \"I do not know which class, but \"\"\"surely\"\"\" (for the model) it\n",
    "is between those classes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50caf0e",
   "metadata": {},
   "source": [
    "Let's load, once again, the \"iris\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa59f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "using RDatasets # used to load the iris dataset\n",
    "\n",
    "data = RDatasets.dataset(\"datasets\", \"iris\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51362967",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "\n",
    "y, X = unpack(data, ==(:Species));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696d1e1",
   "metadata": {},
   "source": [
    "And let's split out data into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test), (y_train, y_test) = partition(\n",
    "    (X, y),\n",
    "    0.8,\n",
    "    rng=13,\n",
    "    shuffle=true,\n",
    "    multi=true\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40712f0c",
   "metadata": {},
   "source": [
    "Our approach works in the following way:\n",
    "- we will further divide our training dataset into n+1 slices, were n is the\n",
    "number of experts (in this first example, just one - so we'll have 2 slices)\n",
    "- then, we will learn a classical (crisp) decision tree, using the first slice\n",
    "of the training set\n",
    "- finally, we will use each of the other `n` slices to train some parameters\n",
    "characterising a \"soft\" version of the learnt decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_dt, X_train_exp), (y_train_dt, y_train_exp) = partition(\n",
    "    (X_train, y_train),\n",
    "    0.4,\n",
    "    rng=42,\n",
    "    shuffle=true,\n",
    "    multi=true\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73427af2",
   "metadata": {},
   "source": [
    "Let's build a classical (crisp) decision tree on the first slice.\n",
    "\n",
    "(Remember: we already shuffled out instances when splitting into train/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8077034",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DecisionTree\n",
    "\n",
    "# Build a standard decision tree (explicitly)\n",
    "dt = build_tree(y_train_dt, Matrix(X_train_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune tree: merge leaves having >= 90% combined purity\n",
    "dt = prune_tree(dt, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ce0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = apply_tree(dt, Matrix(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df418e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff84cc03",
   "metadata": {},
   "source": [
    "Let's try to soften this decision tree!\n",
    "\n",
    "First, we need to define a new structure (we need to add more information about\n",
    "each node of our decision tree); namely, a `ManyExpertDecisionTree`.\n",
    "\n",
    "Not only that: we need to specify a `ManyExpertAlgebra` (more on that in a\n",
    "minute!) specifying a fuzzy logic to use for each expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SoleLogics.ManyValuedLogics\n",
    "\n",
    "mxa = ManyExpertAlgebra(ProductLogic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034cd1f8",
   "metadata": {},
   "source": [
    "The idea is to soften the original decision tree treating each node as a\n",
    "\"membership\" to a \"fuzzy set\" (note that if we use only `true` and `false`, we\n",
    "obtain the original split).\n",
    "\n",
    "Hence, we will leverage membership functions, associating one for each node to\n",
    "each expert: i.e., the parameters I'm learning are the parameters of the chosen\n",
    "function; in our case, we will only use Gaussian functions.\n",
    "\n",
    "For membership functions, we will leverage the `FuzzyLogic.jl` package.\n",
    "\n",
    "Watch out! Even if it is called `FuzzyLogic.jl`, this package offers classical\n",
    "tools (like membership functions) to work with fuzzy sets and system, and it is\n",
    "NOT a package to manipulate mathematical fuzzy logic.\n",
    "\n",
    "Moreover, since we already have `FuzzyLogic` as a type in our naming space, we\n",
    "provide an alias to load the package, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c76bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ManyExpertDecisionTrees: FL   # This is an alias for `FuzzyLogic.jl`\n",
    "using Plots\n",
    "\n",
    "hot = FL.GaussianMF(35.0, 5.0)  # temp>25\n",
    "plot(hot, -10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b3ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold = FL.GaussianMF(10.0, 7.5) # tempâ‰¤25\n",
    "plot(cold, -10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ccfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321e852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "hot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a942c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cold(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e49033f",
   "metadata": {},
   "source": [
    "To soften the decision tree, we use the manify function, specifying:\n",
    "- the original decision tree\n",
    "- the portion of the training set to use\n",
    "- a tuple of kind of \"membership functions\" to use (one for each expert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "medt = manify(dt, X_train_exp, (FL.GaussianMF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mxa = map(eachrow(X_test)) do row\n",
    "    result = ManyExpertDecisionTrees.apply(\n",
    "        medt,\n",
    "        mxa,\n",
    "        Vector{Float64}(row)\n",
    "    )\n",
    "    return length(result) != 1 ? :vague : first(result)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = length(y_test)\n",
    "\n",
    "n_correct = count(i -> y_pred_mxa[i] == y_test[i], 1:n_total)\n",
    "(n_correct / n_total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffac84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vague = count(==(:vague), y_pred_mxa)\n",
    "(n_vague / n_total) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410cf6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wrong = n_total - n_correct - n_vague\n",
    "(n_wrong / n_total) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa411293",
   "metadata": {},
   "source": [
    "Wow, this was lucky! This improved performance!\n",
    "\n",
    "Probably, the heuristic didn't choose the \"best\" attribute at each step: with\n",
    "softening, we can make up for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb67e19",
   "metadata": {},
   "source": [
    "Let's combare different many-expert algebras..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c1e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Combinatorics\n",
    "\n",
    "allexperts = (GodelLogic, LukasiewiczLogic, ProductLogic);\n",
    "\n",
    "# Compute all possible expert compbinations (with replacement)\n",
    "expertcomb = begin\n",
    "    c = Vector{Vector{FuzzyLogic}}()\n",
    "    for i in 1:length(allexperts)\n",
    "        append!(c, collect(Combinatorics.with_replacement_combinations(allexperts, i)))\n",
    "    end\n",
    "    c\n",
    "end;\n",
    "\n",
    "# This is useful to read results later \n",
    "expertcombreadable = map(expertcomb) do experts\n",
    "    result = \"\"\n",
    "    for expert in experts\n",
    "        if (expert === GodelLogic)\n",
    "            result *= \"G\"\n",
    "        end\n",
    "        if (expert === LukasiewiczLogic)\n",
    "            result *= \"L\"\n",
    "        end\n",
    "        if (expert === ProductLogic)\n",
    "            result *= \"P\"\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return result\n",
    "end;\n",
    "\n",
    "correct = [[0.0, 0.0] for _ in 1:length(expertcomb)];\n",
    "wrong = [[0.0, 0.0] for _ in 1:length(expertcomb)];\n",
    "vague = [[0.0, 0.0] for _ in 1:length(expertcomb)];\n",
    "\n",
    "n_runs = 10\n",
    "\n",
    "for i in 1:n_runs\n",
    "    # Partition set into training and validation\n",
    "    X_train, y_train, X_test, y_test = begin\n",
    "        train, test = partition(eachindex(y), 0.8, shuffle=true, rng=i)\n",
    "        X_train, y_train = X[train, :], y[train]\n",
    "        X_test, y_test = X[test, :], y[test]\n",
    "        X_train, y_train, X_test, y_test\n",
    "    end\n",
    "\n",
    "    # Build a standard decision tree\n",
    "    dt = build_tree(y_train, Matrix(X_train))\n",
    "    dt = prune_tree(dt, 0.9)\n",
    "\n",
    "    # For each expert combination, build a ManyExpertDecisionTree \n",
    "    Threads.@threads for k in eachindex(expertcomb)\n",
    "        mf_experts = ntuple(_ -> FL.GaussianMF, length(expertcomb[k]))\n",
    "        MXA = ManyExpertAlgebra(expertcomb[k]...)\n",
    "\n",
    "        medt = manify(dt, X_train, mf_experts...)\n",
    "\n",
    "        y_pred = map(eachrow(X_test)) do row\n",
    "            result = ManyExpertDecisionTrees.apply(\n",
    "                medt,\n",
    "                MXA,\n",
    "                Vector{Float64}(row)\n",
    "            )\n",
    "            return length(result) != 1 ? :vague : first(result)\n",
    "        end\n",
    "\n",
    "        # Extrapolating statistics\n",
    "        n_total = length(y_test)\n",
    "\n",
    "        n_vague = count(==(:vague), y_pred)\n",
    "        pvague = (n_vague / n_total) * 100\n",
    "\n",
    "        n_correct = count(i -> y_pred[i] == y_test[i], 1:n_total)\n",
    "        pcorrect = (n_correct / n_total) * 100\n",
    "\n",
    "        n_wrong = n_total - n_correct - n_vague\n",
    "        pwrong = (n_wrong / n_total) * 100\n",
    "\n",
    "        deltacorrect = (pcorrect - correct[k][1])\n",
    "        correct[k][1] += deltacorrect / i\n",
    "        correct[k][2] += deltacorrect * (pcorrect - correct[k][1])\n",
    "\n",
    "        deltawrong = (pwrong - wrong[k][1])\n",
    "        wrong[k][1] += deltawrong / i\n",
    "        wrong[k][2] += deltawrong * (pwrong - wrong[k][1])\n",
    "\n",
    "        deltavague = (pvague - vague[k][1])\n",
    "        vague[k][1] += deltavague / i\n",
    "        vague[k][2] += deltavague * (pvague - vague[k][1])\n",
    "\n",
    "    end\n",
    "end\n",
    "\n",
    "# Process results: extract means and compute standard deviations (sample std)\n",
    "correct_mean = [x[1] for x in correct]\n",
    "correct_std = [sqrt(x[2] / (n_runs - 1)) for x in correct]\n",
    "\n",
    "wrong_mean = [x[1] for x in wrong]\n",
    "wrong_std = [sqrt(x[2] / (n_runs - 1)) for x in wrong]\n",
    "\n",
    "vague_mean = [x[1] for x in vague]\n",
    "vague_std = [sqrt(x[2] / (n_runs - 1)) for x in vague]\n",
    "\n",
    "df = DataFrame(\n",
    "    experts=expertcombreadable,\n",
    "    correct_mean=correct_mean,\n",
    "    correct_std=correct_std,\n",
    "    wrong_mean=wrong_mean,\n",
    "    wrong_std=wrong_std,\n",
    "    vague_mean=vague_mean,\n",
    "    vague_std=vague_std\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0466f7",
   "metadata": {},
   "source": [
    "**Exercise**: play some more with the iris dataset, trying different\n",
    "combinations of experts. Which is the one that works better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de77e9",
   "metadata": {},
   "source": [
    "**Exercise**: put into practice what you learned using the following dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8177b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "data = DataFrame(CSV.File(\"../datasets/penguins.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7067db",
   "metadata": {},
   "source": [
    "We need a bit of data preprocessing...\n",
    "\n",
    "(We will see more about it tomorrow!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa67024",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Impute\n",
    "\n",
    "data_nomissing = Impute.filter(data; dims=:rows);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c98a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(data_nomissing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec5ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_drop_cols = select!(data_nomissing, Not(:island, :sex))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.8",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
